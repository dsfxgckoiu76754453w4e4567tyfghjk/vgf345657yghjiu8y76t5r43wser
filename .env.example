# Environment Configuration
ENVIRONMENT=dev  # dev, test, prod

# Application
APP_NAME=Shia Islamic Chatbot
APP_VERSION=1.0.0
DEBUG=true
API_HOST=0.0.0.0
API_PORT=8000

# Database (PostgreSQL) - Separate Parameters (Recommended)
# Note: Using port 5433 to avoid conflicts with system PostgreSQL
DATABASE_HOST=localhost
DATABASE_PORT=5433
DATABASE_USER=postgres
DATABASE_PASSWORD=postgres
DATABASE_NAME=shia_chatbot
DATABASE_DRIVER=postgresql+asyncpg
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=10

# Note: Database URL is automatically constructed from the individual parameters above
# Do NOT set DATABASE_URL environment variable as it will override the individual parameters

# Redis
REDIS_URL=redis://localhost:6379/0
REDIS_CACHE_DB=1
REDIS_QUEUE_DB=2

# Celery (Task Queue) - Auto-configured from Redis and Database
# Leave empty for auto-configuration or override for custom setup
CELERY_BROKER_URL=
CELERY_RESULT_BACKEND=
CELERY_TASK_ALWAYS_EAGER=false  # Set to true for synchronous task execution (testing only)

# Flower (Celery Monitoring Dashboard)
FLOWER_USER=admin
FLOWER_PASSWORD=changeme_in_production

# Qdrant (Vector Database)
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=
QDRANT_COLLECTION_NAME=islamic_knowledge

# JWT & Security
JWT_SECRET_KEY=your-secret-key-change-in-production
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# ============================================================================
# Google OAuth Configuration
# ============================================================================
# Get credentials from: https://console.cloud.google.com/apis/credentials
# 1. Create a new project (or select existing)
# 2. Enable Google+ API
# 3. Create OAuth 2.0 credentials
# 4. Add authorized redirect URIs
#
# Unified Account Support: Users with the same email will be treated as ONE user
# whether they sign up/login via email/password OR Google OAuth
GOOGLE_CLIENT_ID=your-google-client-id
GOOGLE_CLIENT_SECRET=your-google-client-secret
GOOGLE_REDIRECT_URI=http://localhost:8000/api/v1/auth/google/callback

# LLM Providers - Legacy (Optional if using OpenRouter)
OPENAI_API_KEY=your-openai-api-key
OPENAI_ORG_ID=

# LLM Providers - Anthropic
ANTHROPIC_API_KEY=your-anthropic-api-key

# LLM Providers - Google
GOOGLE_API_KEY=your-google-api-key
GOOGLE_PROJECT_ID=your-google-project-id
GOOGLE_LOCATION=us-central1

# LLM Providers - Cohere
COHERE_API_KEY=your-cohere-api-key

# OpenRouter - Unified LLM API (RECOMMENDED)
# Sign up at: https://openrouter.ai
# Get API key from: https://openrouter.ai/keys
# Supports 100+ models from OpenAI, Anthropic, Google, Meta, Mistral, and more
OPENROUTER_API_KEY=sk-or-v1-...
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_APP_NAME=WisQu Islamic Chatbot
OPENROUTER_APP_URL=https://wisqu.com

# LLM Configuration (Used by LangGraph service)
LLM_PROVIDER=openrouter  # openrouter (recommended), openai, anthropic
LLM_MODEL=anthropic/claude-3.5-sonnet  # or openai/gpt-4-turbo, google/gemini-pro-1.5, etc.
LLM_TEMPERATURE=0.7  # 0.0-1.0 (0=deterministic, 1=creative)
LLM_MAX_TOKENS=4096

# Embeddings
EMBEDDING_PROVIDER=gemini  # gemini, cohere, openrouter
EMBEDDING_MODEL=gemini-embedding-001
# For OpenRouter embeddings, use models like: openai/text-embedding-3-large
EMBEDDING_DIMENSION=3072

# ============================================
# Reranker (2-Stage Retrieval for RAG)
# ============================================
# Improves retrieval quality by 20-40%
# Stage 1: Vector search retrieves top N candidates (e.g., 50)
# Stage 2: Reranker refines to top K results (e.g., 10)
RERANKER_ENABLED=true
RERANKER_PROVIDER=cohere  # Currently only cohere supported
RERANKER_MODEL=rerank-3.5  # rerank-3.5 or rerank-multilingual-v3.0
# Cohere API Key (required for reranker)
# Sign up at: https://cohere.com/ and get API key from dashboard
# Note: COHERE_API_KEY is already set above, reranker will use it

# Web Search
WEB_SEARCH_ENABLED=true
WEB_SEARCH_PROVIDER=tavily  # tavily, serper, openrouter

# Traditional Search APIs:
# Tavily API (Recommended for LLM applications)
# Sign up at: https://tavily.com
TAVILY_API_KEY=tvly-...
# Serper API (Google Search)
# Sign up at: https://serper.dev
SERPER_API_KEY=...

# OpenRouter Search (Uses web plugin with any model)
# OpenRouter's web plugin works with ANY model via native or Exa search
# Native search: OpenAI, Anthropic, Perplexity (provider's built-in search)
# Exa search: All other models (neural + keyword search)
WEB_SEARCH_MODEL=perplexity/sonar
# Alternative models with native search:
# - perplexity/sonar (fast, balanced, recommended)
# - perplexity/sonar-pro (higher quality)
# - perplexity/sonar-reasoning (reasoning with search)
# - perplexity/sonar-reasoning-pro (best quality with reasoning)
# - openai/gpt-4o (high quality, expensive)
# - openai/gpt-4o-mini (faster, cheaper)
# - anthropic/claude-3.5-sonnet (high quality)
# You can use ANY model - models without native search will use Exa
WEB_SEARCH_TEMPERATURE=0.3  # Lower for more factual responses
WEB_SEARCH_MAX_TOKENS=4096

# Search context size for native search models (low, medium, high)
# Affects quality and cost - higher context = better results but more expensive
WEB_SEARCH_CONTEXT_SIZE=medium  # low, medium, high

# Search engine selection (native, exa, or leave empty for automatic)
# - native: Always use provider's built-in search (only works with supported models)
# - exa: Always use Exa search (works with all models)
# - empty/not set: Auto-detect (uses native if available, otherwise Exa)
WEB_SEARCH_ENGINE=  # Leave empty for automatic selection

# ============================================
# OpenRouter Advanced Features
# ============================================

# Prompt Caching (50-90% cost savings!)
# Supported providers: Anthropic, OpenAI, Gemini, DeepSeek, Groq, Grok, Moonshot
# Caches system prompts, RAG context, and large content automatically
PROMPT_CACHING_ENABLED=true
CACHE_CONTROL_STRATEGY=auto  # auto | manual
# auto: Automatically adds cache breakpoints to large content (>1024 tokens)
# manual: You control cache breakpoints in your code
CACHE_MIN_TOKENS=1024  # Minimum tokens for OpenAI caching

# Model Routing & Fallbacks
# Automatically try fallback models if primary model fails or is unavailable
MODEL_ROUTING_ENABLED=true
DEFAULT_FALLBACK_MODELS=  # Comma-separated list (e.g., "anthropic/claude-3.5-sonnet,openai/gpt-4o")
ROUTING_STRATEGY=auto  # auto | price | latency | uptime
# auto: OpenRouter decides based on availability and quality
# price: Prefer cheaper models
# latency: Prefer faster models
# uptime: Prefer most reliable models
ENABLE_AUTO_ROUTER=false  # Use openrouter/auto model (automatic routing)

# Usage Accounting & Tracking
# Track detailed token usage, costs, and cache savings
USAGE_TRACKING_ENABLED=true
TRACK_USER_IDS=true  # Send user IDs to OpenRouter for cache stickiness

# Image Generation (via OpenRouter)
# Supports Gemini Flash Image, DALL-E, Flux, and more
IMAGE_GENERATION_ENABLED=false  # Set to true to enable
IMAGE_GENERATION_MODELS=google/gemini-2.5-flash-image-preview
# Alternative models:
# - openai/dall-e-3 (high quality but expensive)
# - black-forest-labs/flux-pro (high quality, fast)
# - black-forest-labs/flux-schnell (fast, cheaper)
IMAGE_STORAGE_TYPE=database  # database | s3 | local
IMAGE_MAX_SIZE_MB=10

# Structured Outputs (JSON Schema)
# Force LLM responses to match a specific JSON schema
STRUCTURED_OUTPUTS_ENABLED=true

# Multimodal Processing
# Process PDFs, audio, and images in conversations
PDF_PROCESSING_ENABLED=true
AUDIO_PROCESSING_ENABLED=true
PDF_SKIP_PARSING=false  # Skip parsing for cost control (just send raw PDF)

# Reranker
RERANKER_PROVIDER=cohere  # cohere, vertex
RERANKER_MODEL=rerank-3.5

# ASR (Speech-to-Text)
ASR_PROVIDER=google  # google, whisper
GOOGLE_SPEECH_API_KEY=your-google-speech-api-key

# Chonkie (Chunking)
CHUNKING_STRATEGY=semantic  # semantic, token, sentence, adaptive
CHUNK_SIZE=512
CHUNK_OVERLAP=50

# mem0 (Memory)
MEM0_ENABLED=true
MEM0_COMPRESSION_ENABLED=true

# NeMo Guardrails
GUARDRAILS_ENABLED=true
GUARDRAILS_LLM_PROVIDER=openai
GUARDRAILS_LLM_MODEL=gpt-4o-mini

# Logging & Observability (Standard v2.0)
# Log Level: Controls verbosity of logs
LOG_LEVEL=DEBUG  # DEBUG | INFO | WARNING | ERROR (default: DEBUG in dev, WARNING in prod)

# Log Format: Output format for logs
LOG_FORMAT=colored  # colored | json (default: colored in dev, json in test/prod)

# Log Timestamp: Which timestamps to display
LOG_TIMESTAMP=both  # utc | ir | both (default: both)
# - utc: Only UTC timestamp
# - ir: Only Iranian/Jalali timestamp
# - both: Both UTC and Iranian timestamps

# Log Timestamp Precision: Microsecond precision
LOG_TIMESTAMP_PRECISION=6  # 3 (milliseconds) | 6 (microseconds) (default: 6)

# Log Color: ANSI color output control
LOG_COLOR=auto  # auto | true | false (default: auto)
# - auto: Auto-detect based on TTY, Docker, etc.
# - true: Force colors on
# - false: Force colors off
# Alternative: Set NO_COLOR=1 to disable colors (https://no-color.org/)

# Langfuse (LLM Observability) - Two modes available:
#
# MODE 1: Langfuse Cloud (RECOMMENDED for production)
# Sign up at: https://cloud.langfuse.com (Free tier available)
# Get your keys from: Project Settings > API Keys
LANGFUSE_ENABLED=true
LANGFUSE_PUBLIC_KEY=pk-lf-...  # Get from Langfuse Cloud
LANGFUSE_SECRET_KEY=sk-lf-...  # Get from Langfuse Cloud
LANGFUSE_HOST=https://cloud.langfuse.com  # EU region
# LANGFUSE_HOST=https://us.cloud.langfuse.com  # US region

# MODE 2: Self-hosted Langfuse (requires ClickHouse, MinIO, Redis)
# Uncomment services in docker-compose.yml and see docs/LANGFUSE_SETUP.md
# LANGFUSE_HOST=http://localhost:3001  # Self-hosted URL

# Rate Limiting (requests per minute)
RATE_LIMIT_ANONYMOUS=5
RATE_LIMIT_FREE=10
RATE_LIMIT_PREMIUM=50
RATE_LIMIT_UNLIMITED=1000
RATE_LIMIT_TEST=10000

# CORS
CORS_ORIGINS=http://localhost:3000,http://localhost:8000
CORS_ALLOW_CREDENTIALS=true

# External API
EXTERNAL_API_ENABLED=true
EXTERNAL_API_DEFAULT_RATE_LIMIT=100

# Ahkam Tool Configuration
AHKAM_CACHE_TTL_HOURS=24
AHKAM_FETCH_TIMEOUT_SECONDS=30
AHKAM_MAX_RETRIES=3

# HuggingFace Backup
HUGGINGFACE_TOKEN=
HUGGINGFACE_REPO_ID=

# ============================================================================
# Email Configuration (for OTP and notifications)
# ============================================================================

# Email Provider Selection: "mailgun" (recommended) or "smtp" (fallback)
EMAIL_PROVIDER=mailgun

# Mailgun (Recommended - Production Ready)
# Get your API key from: https://app.mailgun.com/app/account/security/api_keys
# Configure your domain at: https://app.mailgun.com/app/sending/domains
MAILGUN_API_KEY=
MAILGUN_DOMAIN=
MAILGUN_FROM_EMAIL=noreply@wisqu.com
MAILGUN_FROM_NAME=WisQu Islamic Chatbot

# SMTP (Fallback - for development or if Mailgun is not available)
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=
SMTP_PASSWORD=
SMTP_FROM_EMAIL=noreply@example.com
SMTP_FROM_NAME=WisQu Islamic Chatbot

# Super Admin (Initial Setup)
# ⚠️ IMPORTANT: Change these credentials immediately after first login!
SUPER_ADMIN_EMAIL=admin@wisqu.com
SUPER_ADMIN_PASSWORD=ChangeMe123!
