# Environment Configuration
ENVIRONMENT=dev  # dev, test, prod

# Application
APP_NAME=Shia Islamic Chatbot
APP_VERSION=1.0.0
DEBUG=true
API_HOST=0.0.0.0
API_PORT=8000

# Database (PostgreSQL) - Separate Parameters (Recommended)
# Note: Using port 5433 to avoid conflicts with system PostgreSQL
DATABASE_HOST=localhost
DATABASE_PORT=5433
DATABASE_USER=postgres
DATABASE_PASSWORD=postgres
DATABASE_NAME=shia_chatbot
DATABASE_DRIVER=postgresql+asyncpg
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=10

# Note: Database URL is automatically constructed from the individual parameters above
# Do NOT set DATABASE_URL environment variable as it will override the individual parameters

# Database Read Replica (Optional - for read scaling)
DATABASE_READ_REPLICA_ENABLED=false
DATABASE_READ_REPLICA_HOST=
DATABASE_READ_REPLICA_PORT=

# Redis
REDIS_URL=redis://localhost:6379/0
REDIS_CACHE_DB=1
REDIS_QUEUE_DB=2

# ============================================
# Temporal Workflow Engine (Replaces Celery)
# ============================================
# Modern workflow orchestration with durable execution and better observability
TEMPORAL_HOST=localhost:7233
TEMPORAL_NAMESPACE=default
TEMPORAL_TASK_QUEUE=wisqu-dev-queue  # Use wisqu-{environment}-queue pattern
TEMPORAL_ENABLED=true  # Enable Temporal workflows

# Note: Temporal UI is available at http://localhost:8233 when using docker-compose.base.yml

# Qdrant (Vector Database)
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=
QDRANT_COLLECTION_NAME=islamic_knowledge

# JWT & Security
JWT_SECRET_KEY=your-secret-key-change-in-production
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# ============================================================================
# Google OAuth Configuration
# ============================================================================
# Get credentials from: https://console.cloud.google.com/apis/credentials
# 1. Create a new project (or select existing)
# 2. Enable Google+ API
# 3. Create OAuth 2.0 credentials
# 4. Add authorized redirect URIs
#
# Unified Account Support: Users with the same email will be treated as ONE user
# whether they sign up/login via email/password OR Google OAuth
GOOGLE_CLIENT_ID=your-google-client-id
GOOGLE_CLIENT_SECRET=your-google-client-secret
GOOGLE_REDIRECT_URI=http://localhost:8000/api/v1/auth/google/callback

# LLM Providers - Legacy (Optional if using OpenRouter)
OPENAI_API_KEY=your-openai-api-key
OPENAI_ORG_ID=

# LLM Providers - Anthropic
ANTHROPIC_API_KEY=your-anthropic-api-key

# LLM Providers - Google
GOOGLE_API_KEY=your-google-api-key
GOOGLE_PROJECT_ID=your-google-project-id
GOOGLE_LOCATION=us-central1

# LLM Providers - Cohere
COHERE_API_KEY=your-cohere-api-key

# OpenRouter - Unified LLM API (RECOMMENDED)
# Sign up at: https://openrouter.ai
# Get API key from: https://openrouter.ai/keys
# Supports 100+ models from OpenAI, Anthropic, Google, Meta, Mistral, and more
OPENROUTER_API_KEY=sk-or-v1-...
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_APP_NAME=WisQu Islamic Chatbot
OPENROUTER_APP_URL=https://wisqu.com

# LLM Configuration (Used by LangGraph service)
LLM_PROVIDER=openrouter  # openrouter (recommended), openai, anthropic
LLM_MODEL=anthropic/claude-3.5-sonnet  # or openai/gpt-4-turbo, google/gemini-pro-1.5, etc.
LLM_TEMPERATURE=0.7  # 0.0-1.0 (0=deterministic, 1=creative)
LLM_MAX_TOKENS=4096

# Embeddings
EMBEDDING_PROVIDER=gemini  # gemini, cohere, openrouter
EMBEDDING_MODEL=gemini-embedding-001
# For OpenRouter embeddings, use models like: openai/text-embedding-3-large
EMBEDDING_DIMENSION=3072

# ============================================
# Reranker (2-Stage Retrieval for RAG)
# ============================================
# Improves retrieval quality by 20-40%
# Stage 1: Vector search retrieves top N candidates (e.g., 50)
# Stage 2: Reranker refines to top K results (e.g., 10)
RERANKER_ENABLED=true
RERANKER_PROVIDER=cohere  # Currently only cohere supported
RERANKER_MODEL=rerank-3.5  # rerank-3.5 or rerank-multilingual-v3.0
# Cohere API Key (required for reranker)
# Sign up at: https://cohere.com/ and get API key from dashboard
# Note: COHERE_API_KEY is already set above, reranker will use it

# Web Search
WEB_SEARCH_ENABLED=true
WEB_SEARCH_PROVIDER=openrouter  # openrouter (primary), serper

# Serper API (Google Search - Alternative)
# Sign up at: https://serper.dev
SERPER_API_KEY=...

# OpenRouter Search (Uses web plugin with any model)
# OpenRouter's web plugin works with ANY model via native or Exa search
# Native search: OpenAI, Anthropic, Perplexity (provider's built-in search)
# Exa search: All other models (neural + keyword search)
WEB_SEARCH_MODEL=perplexity/sonar
# Alternative models with native search:
# - perplexity/sonar (fast, balanced, recommended)
# - perplexity/sonar-pro (higher quality)
# - perplexity/sonar-reasoning (reasoning with search)
# - perplexity/sonar-reasoning-pro (best quality with reasoning)
# - openai/gpt-4o (high quality, expensive)
# - openai/gpt-4o-mini (faster, cheaper)
# - anthropic/claude-3.5-sonnet (high quality)
# You can use ANY model - models without native search will use Exa
WEB_SEARCH_TEMPERATURE=0.3  # Lower for more factual responses
WEB_SEARCH_MAX_TOKENS=4096

# Search context size for native search models (low, medium, high)
# Affects quality and cost - higher context = better results but more expensive
WEB_SEARCH_CONTEXT_SIZE=medium  # low, medium, high

# Search engine selection (native, exa, or leave empty for automatic)
# - native: Always use provider's built-in search (only works with supported models)
# - exa: Always use Exa search (works with all models)
# - empty/not set: Auto-detect (uses native if available, otherwise Exa)
WEB_SEARCH_ENGINE=  # Leave empty for automatic selection

# ============================================
# OpenRouter Advanced Features
# ============================================

# Prompt Caching (50-90% cost savings!)
# Supported providers: Anthropic, OpenAI, Gemini, DeepSeek, Groq, Grok, Moonshot
# Caches system prompts, RAG context, and large content automatically
PROMPT_CACHING_ENABLED=true
CACHE_CONTROL_STRATEGY=auto  # auto | manual
# auto: Automatically adds cache breakpoints to large content (>1024 tokens)
# manual: You control cache breakpoints in your code
CACHE_MIN_TOKENS=1024  # Minimum tokens for OpenAI caching

# Model Routing & Fallbacks
# Automatically try fallback models if primary model fails or is unavailable
MODEL_ROUTING_ENABLED=true
DEFAULT_FALLBACK_MODELS=  # Comma-separated list (e.g., "anthropic/claude-3.5-sonnet,openai/gpt-4o")
ROUTING_STRATEGY=auto  # auto | price | latency | uptime
# auto: OpenRouter decides based on availability and quality
# price: Prefer cheaper models
# latency: Prefer faster models
# uptime: Prefer most reliable models
ENABLE_AUTO_ROUTER=false  # Use openrouter/auto model (automatic routing)

# Usage Accounting & Tracking
# Track detailed token usage, costs, and cache savings
USAGE_TRACKING_ENABLED=true
TRACK_USER_IDS=true  # Send user IDs to OpenRouter for cache stickiness

# Image Generation (via OpenRouter)
# Supports Gemini Flash Image, DALL-E, Flux, and more
IMAGE_GENERATION_ENABLED=false  # Set to true to enable
IMAGE_GENERATION_MODELS=google/gemini-2.5-flash-image-preview
# Alternative models:
# - openai/dall-e-3 (high quality but expensive)
# - black-forest-labs/flux-pro (high quality, fast)
# - black-forest-labs/flux-schnell (fast, cheaper)
IMAGE_STORAGE_TYPE=database  # database | s3 | local
IMAGE_MAX_SIZE_MB=10

# Structured Outputs (JSON Schema)
# Force LLM responses to match a specific JSON schema
STRUCTURED_OUTPUTS_ENABLED=true

# Multimodal Processing
# Process PDFs, audio, and images in conversations
PDF_PROCESSING_ENABLED=true
AUDIO_PROCESSING_ENABLED=true
PDF_SKIP_PARSING=false  # Skip parsing for cost control (just send raw PDF)

# Chonkie (Chunking)
CHUNKING_STRATEGY=semantic  # semantic, token, sentence, adaptive
CHUNK_SIZE=512
CHUNK_OVERLAP=50

# mem0 (Memory)
MEM0_ENABLED=true
MEM0_COMPRESSION_ENABLED=true

# NeMo Guardrails
GUARDRAILS_ENABLED=true
GUARDRAILS_LLM_PROVIDER=openai
GUARDRAILS_LLM_MODEL=gpt-4o-mini

# Logging & Observability (Standard v2.0)
# Log Level: Controls verbosity of logs
LOG_LEVEL=DEBUG  # DEBUG | INFO | WARNING | ERROR (default: DEBUG in dev, WARNING in prod)

# Log Format: Output format for logs
LOG_FORMAT=colored  # colored | json (default: colored in dev, json in test/prod)

# Log Timestamp: Which timestamps to display
LOG_TIMESTAMP=both  # utc | ir | both (default: both)
# - utc: Only UTC timestamp
# - ir: Only Iranian/Jalali timestamp
# - both: Both UTC and Iranian timestamps

# Log Timestamp Precision: Microsecond precision
LOG_TIMESTAMP_PRECISION=6  # 3 (milliseconds) | 6 (microseconds) (default: 6)

# Log Color: ANSI color output control
LOG_COLOR=auto  # auto | true | false (default: auto)
# - auto: Auto-detect based on TTY, Docker, etc.
# - true: Force colors on
# - false: Force colors off
# Alternative: Set NO_COLOR=1 to disable colors (https://no-color.org/)

# Langfuse (LLM Observability) - Two modes available:
#
# MODE 1: Langfuse Cloud (RECOMMENDED for production)
# Sign up at: https://cloud.langfuse.com (Free tier available)
# Get your keys from: Project Settings > API Keys
LANGFUSE_ENABLED=true
LANGFUSE_PUBLIC_KEY=pk-lf-...  # Get from Langfuse Cloud
LANGFUSE_SECRET_KEY=sk-lf-...  # Get from Langfuse Cloud
LANGFUSE_HOST=https://cloud.langfuse.com  # EU region
# LANGFUSE_HOST=https://us.cloud.langfuse.com  # US region

# MODE 2: Self-hosted Langfuse (requires ClickHouse, MinIO, Redis)
# Uncomment services in docker-compose.yml and see docs/LANGFUSE_SETUP.md
# LANGFUSE_HOST=http://localhost:3001  # Self-hosted URL

# Rate Limiting (requests per minute)
RATE_LIMIT_ANONYMOUS=5
RATE_LIMIT_FREE=10
RATE_LIMIT_PREMIUM=50
RATE_LIMIT_UNLIMITED=1000
RATE_LIMIT_TEST=10000

# CORS
CORS_ORIGINS=http://localhost:3000,http://localhost:8000
CORS_ALLOW_CREDENTIALS=true

# External API
EXTERNAL_API_ENABLED=true
EXTERNAL_API_DEFAULT_RATE_LIMIT=100

# Ahkam Tool Configuration
AHKAM_CACHE_TTL_HOURS=24
AHKAM_FETCH_TIMEOUT_SECONDS=30
AHKAM_MAX_RETRIES=3

# HuggingFace Backup
HUGGINGFACE_TOKEN=
HUGGINGFACE_REPO_ID=

# ============================================================================
# Email Configuration (for OTP and notifications)
# ============================================================================

# Email Provider Selection: "mailgun" (recommended) or "smtp" (fallback)
EMAIL_PROVIDER=mailgun

# Mailgun (Recommended - Production Ready)
# Get your API key from: https://app.mailgun.com/app/account/security/api_keys
# Configure your domain at: https://app.mailgun.com/app/sending/domains
MAILGUN_API_KEY=
MAILGUN_DOMAIN=
MAILGUN_FROM_EMAIL=noreply@wisqu.com
MAILGUN_FROM_NAME=WisQu Islamic Chatbot

# SMTP (Fallback - for development or if Mailgun is not available)
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=
SMTP_PASSWORD=
SMTP_FROM_EMAIL=noreply@example.com
SMTP_FROM_NAME=WisQu Islamic Chatbot

# Super Admin (Initial Setup)
# ⚠️ IMPORTANT: Change these credentials immediately after first login!
SUPER_ADMIN_EMAIL=admin@wisqu.com
SUPER_ADMIN_PASSWORD=ChangeMe123!

# ============================================================================
# MinIO Object Storage Configuration
# ============================================================================
# MinIO provides S3-compatible object storage for files, images, audio, and backups
# Docker setup: MinIO runs on port 9000, Console on port 9001
MINIO_ENABLED=true
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_SECURE=false  # True for HTTPS in production
MINIO_REGION=us-east-1
MINIO_PUBLIC_URL=http://localhost:9000  # Public URL for generated URLs

# MinIO Bucket Names (automatically prefixed with environment)
MINIO_BUCKET_IMAGES=wisqu-images  # AI-generated images (public)
MINIO_BUCKET_DOCUMENTS=wisqu-documents  # RAG docs, user PDFs
MINIO_BUCKET_AUDIO_RESOURCES=wisqu-audio-resources  # Quran, Mafatih, Duas (public)
MINIO_BUCKET_AUDIO_USER=wisqu-audio-user  # User voice messages (private)
MINIO_BUCKET_AUDIO_TRANSCRIPTS=wisqu-audio-transcripts  # ASR processed audio
MINIO_BUCKET_UPLOADS=wisqu-uploads  # General uploads, ticket attachments
MINIO_BUCKET_TEMP=wisqu-temp  # Temporary processing
MINIO_BUCKET_BACKUPS=wisqu-backups  # System backups

# ============================================================================
# Storage Limits & User Quotas
# ============================================================================
# File Size Limits (in MB)
STORAGE_MAX_FILE_SIZE_MB=50  # Max file size for general uploads
STORAGE_MAX_IMAGE_SIZE_MB=10  # Max image size
STORAGE_MAX_AUDIO_SIZE_MB=25  # Max audio file size
STORAGE_MAX_DOCUMENT_SIZE_MB=20  # Max document size (PDF, DOCX, etc.)

# User Storage Quotas by Plan (in MB)
STORAGE_QUOTA_FREE=100  # 100MB for free tier
STORAGE_QUOTA_PREMIUM=5120  # 5GB for premium
STORAGE_QUOTA_UNLIMITED=51200  # 50GB for unlimited

# ============================================================================
# ASR (Automatic Speech Recognition) Configuration
# ============================================================================
ASR_ENABLED=true
ASR_PROVIDER=gemini  # google, openai, whisper, gemini
ASR_LANGUAGE=fa-IR  # Persian/Farsi (primary language)
ASR_ALTERNATIVE_LANGUAGES=ar-SA,en-US  # Arabic, English for fallback
ASR_MAX_AUDIO_DURATION_SECONDS=600  # 10 minutes max
GOOGLE_ASR_CREDENTIALS_PATH=  # Path to Google Cloud JSON credentials (optional if using default auth)

# Gemini ASR Settings (when ASR_PROVIDER=gemini)
GEMINI_MODEL=gemini-2.0-flash-exp  # Model for audio transcription (<20MB inline, >20MB upload)

# ============================================================================
# Environment-Specific Settings (Auto-configured by environment)
# ============================================================================
# Data Retention (days) - Auto-set based on ENVIRONMENT
# dev/test: 30 days, stage: 90 days, prod: 365 days
ENVIRONMENT_DATA_RETENTION_DAYS=30
ENVIRONMENT_ALLOW_TEST_ACCOUNTS=true
ENVIRONMENT_AUTO_CLEANUP_ENABLED=true
ENVIRONMENT_CLEANUP_SCHEDULE=0 2 * * *  # Cron: Daily at 2 AM

# ============================================================================
# Promotion Settings (Environment promotion workflow)
# ============================================================================
PROMOTION_ENABLED=true
PROMOTION_REQUIRE_APPROVAL=true
PROMOTION_ALLOWED_PATHS=dev->stage,stage->prod,dev->prod  # Comma-separated promotion paths
PROMOTION_MAX_ITEMS_PER_BATCH=100
PROMOTION_ROLLBACK_WINDOW_HOURS=24
